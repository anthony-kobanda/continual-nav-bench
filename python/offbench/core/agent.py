import numpy as np
import random as rd
import torch

from .policy import Policy
from .data import Frame
from abc import ABC, abstractmethod
from offbench.utils.imports import instantiate_class
from omegaconf import DictConfig
from torch.nn.parameter import Parameter
from typing import Any, Dict, Iterator, List, Optional, Union



class Agent:

    """
    Agent integrates a policy with an interface for interaction,
    facilitating actions based on input frames and managing policy states.

    Args:
        policy_cfg (DictConfig): Configuration for the policy instance.
        seed (Optional[int]): Seed for random number generators. Defaults to None.
        device (Union[torch.device, str]): The device to use ('cpu', 'cuda'). Defaults to 'cpu'.
    """

    def __init__(
        self,
        agent_id: str,
        policy_cfg: DictConfig,
        seed: Optional[int] = None,
        device: Union[torch.device, str] = "cpu") -> None:

        self._agent_id: str = agent_id
        self._seed: int = seed
        self._device: Union[torch.device, str] = device

        self._policy: Policy = instantiate_class(policy_cfg)
        self._generator: torch.Generator = torch.Generator(device=self._device)

        self.seeding(self._seed)
        self.to(self._device)
    
    def seeding(self,seed: Optional[int] = None) -> None:
        """
        Seeds the random number generators.

        Args:
            seed (Optional[int]): Optional seed value to reproduce experiments. Defaults to None.
        
        Returns:
            (None): Nothing.
        """
        self._seed = seed
        if not (self._seed is None): 
            np.random.seed(self._seed)
            rd.seed(self._seed)
            torch.manual_seed(self._seed)
            if torch.cuda.is_available(): torch.cuda.manual_seed_all(self._seed)
            self._generator.manual_seed(self._seed)
        else:
            np.random.seed()
            rd.seed()
            torch.manual_seed(rd.randint(0,10000000))
            if torch.cuda.is_available(): torch.cuda.manual_seed_all(rd.randint(0,10000000))
            self._generator.manual_seed(rd.randint(0,10000000))

    def get_id(self) -> str:
        """
        Retrieves the agent ID.

        Returns:
            (str): The ID of the agent.
        """
        return self._agent_id
    
    def set_normalizers(self, normalize_values: Dict[str, torch.Tensor], **kwargs) -> "Agent":
        """
        Sets the normalizers for the policy.

        Args:
            normalize_values (Dict[str, torch.Tensor]): The values to normalize the input data.
        """
        self._policy = self._policy.set_normalizers(normalize_values, **kwargs)
        return self

    def reset(self, seed: Optional[int] = None, **kwargs) -> "Agent":
        """
        Resets the interface and the policy with a new seed and any additional arguments.

        Args:
            seed (Optional[int]): New seed for random number generators. Defaults to None.

        Returns:
            (Agent): The instance of this agent after reset.
        """
        self.seeding(seed)
        self._policy = self._policy.reset(self._seed, **kwargs)
        return self

    def set_train_mode(self, **kwargs) -> "Agent":
        """
        Sets the policy to training mode.

        Returns:
            (Agent): The agent with its policy set to training mode.
        """
        self._policy = self._policy.set_train_mode(**kwargs)
        return self

    def set_eval_mode(self, **kwargs) -> "Agent":
        """
        Sets the policy to evaluation mode.

        Returns:
            (Agent): The agent with its policy set to evaluation mode.
        """
        self._policy = self._policy.set_eval_mode(**kwargs)
        return self

    @torch.no_grad()
    def act(self, frame: Frame, **kwargs) -> Frame:
        """
        Generates an action based on the input frame.

        Args:
            frame (Frame): The input frame for generating an action.

        Returns:
            (Frame): The output frame containing the action generated by the policy.
        """
        outputs = self._policy.forward(inputs=frame, generator=self._generator, **kwargs)
        return Frame(
            observation=None,
            action=outputs,
            reward=None,
            done=None,
            truncated=None,
            timestep=None
        )
    
    def policy_parameters(self) -> Iterator[Parameter]:
        """
        Returns an iterator over the policy's parameters.

        Returns:
            (Iterator[Parameter]): An iterator over the policy's parameters.
        """
        return self._policy.parameters()

    def size(self) -> float:
        """
        Calculates the total size of the policy's parameters and buffers in megabytes (MB).

        Returns:
            (float): The total size of the policy in MB.
        """
        return self._policy.size()

    def inference_size(self) -> float:
        """
        Calculates the size of the policy's parameters and buffers used for inference in MB.

        Returns:
            (float): The size of the policy for inference in MB.
        """
        return self._policy.inference_size()

    def to(self, device: Union[torch.device, str]) -> "Agent":
        """
        Moves the policy to a specified device (CPU or GPU).

        Args:
            device (Union[torch.device, str]): The target device.

        Returns:
            (Agent): The agent with its policy moved to the specified device.
        """
        self._device = device
        self._policy = self._policy.to(self._device)
        self._generator = torch.Generator(device=self._device)
        self.seeding(self._seed)
        return self
    
    def update(self, batch: Dict[str, torch.Tensor], gradient_step: int, **kwargs) -> Dict[str,torch.Tensor]:
        """
        Performs a policy update based on the input batch (one gradient step).

        Args:
            batch (Dict[str, torch.Tensor]): The batch of inputs for computing the loss.
            gradient_step (int): The current gradient step.

        Returns:
            (Dict[str,torch.Tensor]): The computed loss(es) for the policy.
        """
        return self._policy.update(batch, generator=self._generator, gradient_step=gradient_step, **kwargs)

    def __getstate__(self) -> Dict[str, Any]:
        """
        Gets the state of the agent.

        Returns:
            (Dict[str, Any]): The state of the agent.
        """
        state = self.__dict__.copy()
        if "_generator" in state: 
            del state["_generator"]
        return state

    def __setstate__(self, state: Dict[str, Any]) -> None:
        """
        Sets the state of the agent.

        Args:
            state (Dict[str, Any]): The state of the agent.
        """
        self.__dict__.update(state)
        self._generator = torch.Generator(device=self._device)
        self.seeding(self._seed)



class AgentsDB(ABC):

    """
    Abstract base class for a database of agents.
    """

    @abstractmethod
    def __init__(self) -> None:
        raise NotImplementedError

    @abstractmethod
    def __contains__(self, agent_id: str) -> bool:
        """
        Check if an agent is in the database.

        Args:
            agent_id (str): The ID of the agent.

        Returns:
            (bool): True if the agent is in the database, False otherwise.
        """
        raise NotImplementedError

    @abstractmethod
    def __repr__(self) -> str:
        """
        Get a string representation of the database.

        Returns:
            (str): A string representation of the database.
        """
        raise NotImplementedError

    @abstractmethod
    def add_agent(self, agent: Agent, agent_id: str, agent_stage: int) -> None:
        """
        Add an agent to the database with a given ID and stage.

        Args:
            agent (Agent): The agent to add.
            agent_id (str): The ID of the agent.
            agent_stage (int): The stage of the agent.
        """
        raise NotImplementedError

    @abstractmethod
    def delete_agent(self, agent_id: str, agent_stage: int = None) -> None:
        """
        Delete an agent from the database (all stages if None is provided).

        Args:
            agent_id (str): The ID of the agent.
            agent_stage (int): The stage of the agent (if None, delete all stages). Default to None.

        Returns:
            (None): Nothing.
        """
        raise NotImplementedError

    @abstractmethod
    def get_ids(self) -> List[str]:
        """
        Get the IDs of the agents in the database.

        Returns:
            (List[str]): The IDs of the agents in the database.
        """
        raise NotImplementedError

    @abstractmethod
    def get(self, agent_id: str, agent_stage: int) -> Agent:
        """
        Get an agent by ID and stage.

        Args:
            agent_id (str): The ID of the agent.
            agent_stage (int): The stage of the agent.

        Returns:
            (Agent): The agent with the given ID and stage.
        """
        raise NotImplementedError

    @abstractmethod
    def get_first_stage(self, agent_id: str) -> "Agent":
        """
        Get the first stage of an agent with a given ID.

        Args:
            agent_id (str): The ID of the agent.

        Returns:
            (Agent): The first agent with the given ID.
        """
        raise NotImplementedError

    @abstractmethod
    def get_last_stage(self, agent_id: str) -> "Agent":
        """
        Get the last stage of an agent with a given ID.

        Args:
            agent_id (str): The ID of the agent.

        Returns:
            (Agent): The last agent with the given ID.
        """
        raise NotImplementedError
    
    @abstractmethod
    def stages(self, agent_id: str) -> List[int]:
        """
        Returns the stages of the specified agent.

        Args:
            agent_id (str): The agent ID.

        Returns:
            (List[int]): The stages of the specified agent
        """
        raise NotImplementedError
