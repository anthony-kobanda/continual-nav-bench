#? the descriptions of most of the following parameters are given in the files they're used

#############
# HYDRA CFG #
#############

hydra:

  launcher:
    n_jobs: 3

  sweeper:
    params:
      ++seed: 100, 200, 300, 400, 500, 600, 700, 800
      ++maze: maze_0, maze_1, maze_2, maze_3, maze_4, maze_5, maze_6, maze_7

defaults:
  - _self_
  - override hydra/launcher: joblib

################
# TRAINING CFG #
################

# general
#########

maze: maze_0

seed: 0
verbose: true

# configuration of the task
###########################

task_cfg:
  task_name: simpletown-${maze}
  episodes_db_cfg:
    classname: offbench.data.episodes_db.GodotGoalEpisodesDB
    env: ${task_cfg.task_name}

# configuration of the algorithm
################################

algo_cfg:

  algorithm: 
    classname: offbench.algorithms.singletask.bc.bc_base:BC_Algorithm

  # general
  verbose: ${verbose}
  logging_frequency: 1000
  log_infos: false

  # training
  train_seed: ${seed}
  train_device: cpu
  train_gradient_steps: 100000
  train_save_every: 25000
  train_reset_args: null
  train_mode_args: null

  # evaluation
  eval_agent: true
  eval_seed: ${seed}
  eval_device: cpu
  eval_every: 25000
  eval_n_episodes: 100
  eval_max_episode_steps: 300
  eval_gamma: 0.99
  eval_reset_args:
    batch_size: 20
    exe_type: windows
  eval_mode_args:
    stochastic: false
  
  # visualization
  visu_seed: ${seed}
  visu_device: cpu
  visu_n_episodes: 100
  visu_max_episode_steps: 300
  visu_reset_args:
    batch_size: 20
    exe_type: windows
  visu_mode_args:
    stochastic: false
  
  # sampler
  sampler_cfg:
    classname: offbench.algorithms.singletask.bc.samplers.her_sampler:BC_HER_Sampler
    her_strategy: exponential
    her_strategy_params:
      temperature: 20.0
    batch_size: 64                                            #? batch size
    context_size: 0                                           #? how many previous frames to use (0 == just use the current frame)
    percentage_episodes: 1.0                                  #? how much of the dataset to use (1.0 == 100%)
    padding_size_begin: ${algo_cfg.sampler_cfg.context_size}  #? padding size at the beginning of the episodes
    padding_size_end: 0                                       #? padding size at the end of the episodes
    padding_value_begin: 0                                    #? padding value at the beginning of the episodes
    padding_value_end: 0                                      #? padding value at the end of the episodes
    reward_scale_w: 1.0                                       #? reward scale weight
    reward_scale_b: 0.0                                       #? reward scale bias
    seed: ${seed}                                             #? seed for the sampler                   
    device: ${algo_cfg.train_device}                          #? device for the sampler

# configuration of the agent
############################

agent_cfg:

  classname: offbench.core.agent:Agent

  agent_id: gcbc_base_mlp

  policy_cfg:

    classname: offbench.algorithms.singletask.bc.policies.base_mlp:MLP

    env: ${task_cfg.task_name}
    seed: ${seed}

    # all networks
    dropout: 0.1
    init_scaling: 0.1
    device: ${algo_cfg.train_device}

    # policy network
    hidden_sizes: [256, 256]
  
    # policy optimizer
    optimizer_cfg:
      classname: torch.optim.Adam
      lr: 3e-4

    # policy scheduler
    scheduler_cfg:
      classname: torch.optim.lr_scheduler.CosineAnnealingLR
      T_max: ${algo_cfg.train_gradient_steps}
  